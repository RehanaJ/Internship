{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2cca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Rank  \\\n",
      "0                \"Baby Shark Dance\"[7]   \n",
      "1                      \"Despacito\"[10]   \n",
      "2                  \"See You Again\"[20]   \n",
      "3                 \"Gangnam Style\"⁂[31]   \n",
      "4                          \"Baby\"*[69]   \n",
      "5                    \"Bad Romance\"[73]   \n",
      "6          \"Charlie Bit My Finger\"[77]   \n",
      "7             \"Evolution of Dance\"[79]   \n",
      "8                \"Girlfriend\"‡[81][82]   \n",
      "9             \"Evolution of Dance\"[79]   \n",
      "10      \"Music Is My Hot Hot Sex\"‡[87]   \n",
      "11           \"Evolution of Dance\"*[79]   \n",
      "12    \"Pokémon Theme Music Video\"‡[92]   \n",
      "13      \"Myspace – The Movie\"‡[97][98]   \n",
      "14           \"Phony Photo Booth\"‡[101]   \n",
      "15   \"The Chronic of Narnia Rap\"‡[107]   \n",
      "16  \"Ronaldinho: Touch of Gold\"‡*[110]   \n",
      "17                  \"I/O Brush\"‡*[116]   \n",
      "\n",
      "                                           Name         Artist  \\\n",
      "0   Pinkfong Baby Shark - Kids' Songs & Stories  7,046,700,000   \n",
      "1                                    Luis Fonsi  2,993,700,000   \n",
      "2                                   Wiz Khalifa  2,894,000,000   \n",
      "3                                           Psy    803,700,000   \n",
      "4                                 Justin Bieber    245,400,000   \n",
      "5                                     Lady Gaga    178,400,000   \n",
      "6                                         HDCYT    128,900,000   \n",
      "7                                Judson Laipply    118,900,000   \n",
      "8                                   RCA Records     92,600,000   \n",
      "9                                Judson Laipply     78,400,000   \n",
      "10                               CLARUSBARTEL72     76,600,000   \n",
      "11                               Judson Laipply     10,600,000   \n",
      "12                                        Smosh      4,300,000   \n",
      "13                                       eggtea      2,700,000   \n",
      "14                                    mugenized      3,400,000   \n",
      "15                                  youtubedude      2,300,000   \n",
      "16                                   Nikesoccer        255,000   \n",
      "17                                       larfus        247,000   \n",
      "\n",
      "          Upload Date             Views  \n",
      "0       June 17, 2016   November 2 2020  \n",
      "1    January 12, 2017     August 4 2017  \n",
      "2       April 6, 2015      July 10 2017  \n",
      "3       July 15, 2012  November 24 2012  \n",
      "4   February 19, 2010      July 16 2010  \n",
      "5   November 24, 2009     April 14 2010  \n",
      "6        May 22, 2007   October 25 2009  \n",
      "7       April 6, 2006        May 2 2009  \n",
      "8   February 27, 2007      July 17 2008  \n",
      "9       April 6, 2006     March 15 2008  \n",
      "10      April 9, 2007      March 1 2008  \n",
      "11      April 6, 2006       May 19 2006  \n",
      "12  November 28, 2005     March 12 2006  \n",
      "13   January 31, 2006  February 18 2006  \n",
      "14   December 1, 2005   January 21 2006  \n",
      "15  December 18, 2005    January 9 2006  \n",
      "16   October 21, 2005   October 31 2005  \n",
      "17    October 5, 2005   October 29 2005  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "table_xpath = \"//table[@class='wikitable sortable jquery-tablesorter']\"\n",
    "table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, table_xpath)))\n",
    "\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "video_details = []\n",
    "\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if len(cols) > 0:\n",
    "        rank = cols[0].text\n",
    "        name = cols[1].text\n",
    "        artist = cols[2].text\n",
    "        upload_date = cols[3].text\n",
    "        views = cols[4].text.replace(\",\", \"\")  # Remove commas from the view count\n",
    "        video_details.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Artist\": artist,\n",
    "            \"Upload Date\": upload_date,\n",
    "            \"Views\": views\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(video_details)\n",
    "\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "table_xpath = \"//table[@class='fixtures-table']\"\n",
    "table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, table_xpath)))\n",
    "\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "fixture_details = []\n",
    "\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if len(cols) > 0:\n",
    "        series = cols[0].text\n",
    "        place = cols[1].text\n",
    "        date = cols[2].text\n",
    "        time = cols[3].text\n",
    "        fixture_details.append({\n",
    "            \"Series\": series,\n",
    "            \"Place\": place,\n",
    "            \"Date\": date,\n",
    "            \"Time\": time\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(fixture_details)\n",
    "\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "economy_tab_xpath = \"//a[@href='#economy']\"\n",
    "economy_tab = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, economy_tab_xpath)))\n",
    "economy_tab.click()\n",
    "\n",
    "state_wise_gdp_xpath = \"//a[@href='/economy/state-wise-gdp-of-india.php']\"\n",
    "state_wise_gdp = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, state_wise_gdp_xpath)))\n",
    "state_wise_gdp.click()\n",
    "\n",
    "table_xpath = \"//table[@class='table table-striped table-bordered']\"\n",
    "table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, table_xpath)))\n",
    "\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "gdp_details = []\n",
    "\n",
    "for row in rows[1:]: \n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if len(cols) > 0:\n",
    "        rank = cols[0].text\n",
    "        state = cols[1].text\n",
    "        gsdp_18_19 = cols[2].text\n",
    "        gsdp_19_20 = cols[3].text\n",
    "        share_18_19 = cols[4].text\n",
    "        gdp_usd = cols[5].text\n",
    "        gdp_details.append({\n",
    "            \"Rank\": rank,\n",
    "            \"State\": state,\n",
    "            \"GSDP (18-19) - at current prices\": gsdp_18_19,\n",
    "            \"GSDP (19-20) - at current prices\": gsdp_19_20,\n",
    "            \"Share (18-19)\": share_18_19,\n",
    "            \"GDP ($ billion)\": gdp_usd\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(gdp_details)\n",
    "\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(\"https://github.com/RehanaJ/Internship\")\n",
    "\n",
    "trending_repos = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".repo-list-item\"))\n",
    ")\n",
    "\n",
    "for repo in trending_repos:\n",
    "    # A) Repository title\n",
    "    title = repo.find_element_by_css_selector(\".lh-condensed\").text.strip()\n",
    "    \n",
    "    # B) Repository description\n",
    "    description = repo.find_element_by_css_selector(\".col-9.text-gray.mb-2\").text.strip()\n",
    "    \n",
    "    # C) Contributors count\n",
    "    contributors = repo.find_element_by_css_selector(\".text-gray.mr-1\").text.strip()\n",
    "    contributors_count = int(contributors.split(\" \")[0])\n",
    "    \n",
    "    # D) Language used\n",
    "    language = repo.find_element_by_css_selector(\".repo-language-color\").get_attribute(\"aria-label\")\n",
    "    \n",
    "    print(f\"**{title}**\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Contributors: {contributors_count}\")\n",
    "    print(f\"Language: {language}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c3ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Data saved to billboard_hot_100.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def get_page_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_billboard_top_100(url):\n",
    "    soup = get_page_data(url)\n",
    "    songs = []\n",
    "    for song in soup.find_all('li', class_='chart-list__item'):\n",
    "        song_data = {}\n",
    "        song_data['rank'] = song.find('span', class_='chart-element__rank__number').text.strip()\n",
    "        song_data['title'] = song.find('span', class_='chart-element__information__song').text.strip()\n",
    "        song_data['artist'] = song.find('span', class_='chart-element__information__artist').text.strip()\n",
    "        song_data['last_week_rank'] = song.find('span', class_='chart-element__meta__text--last-week').text.strip()\n",
    "        song_data['peak_rank'] = song.find('span', class_='chart-element__meta__text--peak').text.strip()\n",
    "        song_data['weeks_on_board'] = song.find('span', class_='chart-element__meta__text--weeks').text.strip()\n",
    "        songs.append(song_data)\n",
    "    return songs\n",
    "\n",
    "url = 'https://www.billboard.com/charts/hot-100'\n",
    "songs = scrape_billboard_top_100(url)\n",
    "\n",
    "with open('billboard_hot_100.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['rank', 'title', 'artist', 'last_week_rank', 'peak_rank', 'weeks_on_board']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for song in songs:\n",
    "        writer.writerow(song)\n",
    "\n",
    "print('Scraping complete! Data saved to billboard_hot_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".book-list\")))\n",
    "\n",
    "books = []\n",
    "for book in driver.find_elements(By.CSS_SELECTOR, \".book-list li\"):\n",
    "    book_data = {}\n",
    "    book_data['book_name'] = book.find_element(By.CSS_SELECTOR, \"h2\").text.strip()\n",
    "    book_data['author_name'] = book.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
    "    book_data['volumes_sold'] = book.find_element(By.CSS_SELECTOR, \".sales-figure\").text.strip()\n",
    "    book_data['publisher'] = book.find_element(By.CSS_SELECTOR, \".publisher\").text.strip()\n",
    "    book_data['genre'] = book.find_element(By.CSS_SELECTOR, \".genre\").text.strip()\n",
    "    books.append(book_data)\n",
    "\n",
    "with open('highest_selling_novels.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['book_name', 'author_name', 'volumes_sold', 'publisher', 'genre']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for book in books:\n",
    "        writer.writerow(book)\n",
    "\n",
    "print('Scraping complete! Data saved to highest_selling_novels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "\n",
    "PATH = r\"C:\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "popular_show = driver.find_element_by_link_text('Most Popular Shows')\n",
    "popular_show.click()\n",
    "\n",
    "shows = []\n",
    "for show in driver.find_elements(By.CSS_SELECTOR, \".lister-item\"):\n",
    "    show_data = {}\n",
    "    show_data['name'] = show.find_element(By.CSS_SELECTOR, \".lister-item-header a\").text.strip()\n",
    "    show_data['year_span'] = show.find_element(By.CSS_SELECTOR, \".lister-item-year\").text.strip()\n",
    "    show_data['genre'] = show.find_element(By.CSS_SELECTOR, \".genre\").text.strip()\n",
    "    show_data['run_time'] = show.find_element(By.CSS_SELECTOR, \".runtime\").text.strip()\n",
    "    show_data['ratings'] = show.find_element(By.CSS_SELECTOR, \".rating\").text.strip()\n",
    "    show_data['votes'] = show.find_element(By.CSS_SELECTOR, \".votes\").text.strip()\n",
    "    shows.append(show_data)\n",
    "\n",
    "with open('most_watched_tv_series.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['name', 'year_span', 'genre', 'run_time', 'ratings', 'votes']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for show in shows:\n",
    "        writer.writerow(show)\n",
    "\n",
    "print('Scraping complete! Data saved to most_watched_tv_series.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75187536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "PATH = r\"C:\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "datasets_link = driver.find_element_by_link_text('Datasets')\n",
    "datasets_link.click()\n",
    "\n",
    "datasets = []\n",
    "for dataset in driver.find_elements(By.CSS_SELECTOR, \".dataset\"):\n",
    "    dataset_data = {}\n",
    "    dataset_data['name'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-name\").text.strip()\n",
    "    dataset_data['data_type'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-type\").text.strip()\n",
    "    dataset_data['task'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-task\").text.strip()\n",
    "    attribute_type = dataset.find_element(By.CSS_SELECTOR, \".dataset-attributes\").text.strip()\n",
    "    if 'Numeric' in attribute_type:\n",
    "        dataset_data['attribute_type'] = 'Numeric'\n",
    "    elif 'Categorical' in attribute_type:\n",
    "        dataset_data['attribute_type'] = 'Categorical'\n",
    "    else:\n",
    "        dataset_data['attribute_type'] = 'Mixed'\n",
    "    dataset_data['no_of_instances'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-instances\").text.strip()\n",
    "    dataset_data['no_of_attributes'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-attributes\").text.strip().split()[0]\n",
    "    dataset_data['year'] = dataset.find_element(By.CSS_SELECTOR, \".dataset-year\").text.strip()\n",
    "    datasets.append(dataset_data)\n",
    "\n",
    "with open('uci_datasets.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['name', 'data_type', 'task', 'attribute_type', 'no_of_instances', 'no_of_attributes', 'year']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for dataset in datasets:\n",
    "        writer.writerow(dataset)\n",
    "\n",
    "print('Scraping complete! Data saved to uci_datasets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
