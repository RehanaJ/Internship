{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcb32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\rehana\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb725a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\rehana\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rehana\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7ef15",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9900e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating    Year\n",
      "0                     Ship of Theseus      8  (2012)\n",
      "1                              Iruvar    8.4  (1997)\n",
      "2                     Kaagaz Ke Phool    7.8  (1959)\n",
      "3   Lagaan: Once Upon a Time in India    8.1  (2001)\n",
      "4                     Pather Panchali    8.2  (1955)\n",
      "..                                ...    ...     ...\n",
      "95                        Apur Sansar    8.4  (1959)\n",
      "96                        Kanchivaram    8.2  (2008)\n",
      "97                    Monsoon Wedding    7.3  (2001)\n",
      "98                              Black    8.1  (2005)\n",
      "99                            Deewaar      8  (1975)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.imdb.com/list/ls056092300/\"\n",
    "page=requests.get(url)\n",
    "soup=BeautifulSoup(page.content,\"html.parser\")\n",
    "movies=[]\n",
    "movies_container =soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "for movie in movies_container :\n",
    "    name = movie.find('a').text.strip()\n",
    "    \n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    \n",
    "    year = movie.find('span', class_='lister-item-year').text.strip()\n",
    "    \n",
    "    movies.append((name, rating, year))\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(movies, columns=['Name', 'Rating', 'Year'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7169b2",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65ff9e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Date, Content, Video URL, Video Title, Likes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    post_items = soup.select('.post-item')\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    \n",
    "    for post in post_items:\n",
    "        \n",
    "        title = post.select_one('.title').get_text(strip=True)\n",
    "        date = post.select_one('.date').get_text(strip=True)\n",
    "\n",
    "        content = post.select_one('.content').get_text(strip=True)\n",
    "\n",
    "        match = re.search(r'https?://www\\.youtube\\.com/watch\\?v=([-\\w]+)', content)\n",
    "        if match:\n",
    "            video_url = f'https://www.youtube.com/watch?v={match.group(1)}'\n",
    "\n",
    "            \n",
    "            video_response = requests.get(video_url)\n",
    "\n",
    "            \n",
    "            if video_response.status_code == 200:\n",
    "                \n",
    "                video_soup = BeautifulSoup(video_response.text, 'html.parser')\n",
    "\n",
    "                video_title = video_soup.select_one('.title-and-badge h1').get_text(strip=True)\n",
    "                likes = video_soup.select_one('.ytp-social-share-container .ytp-social-item-button-renderer:nth-child(2) .ytp-social-item-button-renderer-shared-child')\n",
    "                if likes:\n",
    "                    likes = likes.get_text(strip=True).replace(',', '').replace(' ', '')\n",
    "                else:\n",
    "                    likes = 0\n",
    "\n",
    "                \n",
    "                posts.append((title, date, content, video_url, video_title, likes))\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['Title', 'Date', 'Content', 'Video URL', 'Video Title', 'Likes'])\n",
    "    \n",
    "    print(df.head())  \n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ea40f",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d16d7d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (2203633875.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    location = result.find(\"div\", {class_= \"px-1.5p py-0.5p border-t-1 border-cardbordercolor\"}).get_text().strip()\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL with the locality name\n",
    "url = \"https://www.nobroker.in/search?q={}\"\n",
    "\n",
    "# Define the localities\n",
    "localities = [\"Indira Nagar\", \"Jayanagar\", \"Rajaji Nagar\"]\n",
    "\n",
    "# Initialize an empty list to store the house details\n",
    "house_details = []\n",
    "\n",
    "# Loop through the localities\n",
    "for locality in localities:\n",
    "    # Send an HTTP request to the webpage for the current locality\n",
    "    response = requests.get(url.format(locality))\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the house details\n",
    "    results = soup.find_all(\"div\", {\"class\": \"card\"})\n",
    "    for result in results:\n",
    "        title = result.find(\"h2\", class_=\"heading-6 flex items-center font-semi-bold m-0\").get_text().strip()\n",
    "        location = result.find(\"div\", {class_= \"px-1.5p py-0.5p border-t-1 border-cardbordercolor\"}).get_text().strip()\n",
    "        area = result.find(\"div\", {\"class\": \"nb__3oNyC\"}).get_text().strip()\n",
    "        price = result.find(\"div\", {class_= \"flex\"}).get_text().strip()\n",
    "\n",
    "        # Append the house details to the list\n",
    "        house_details.append({\n",
    "            \"title\": title,\n",
    "            \"location\": location,\n",
    "            \"area\": area,\n",
    "            \"price\": price\n",
    "        })\n",
    "\n",
    "# Print the house details\n",
    "for house in house_details:\n",
    "    print(house)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc7e04",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0daccbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product name: Bewakoof®Women's Brown Never Give Up Graphic Printed Oversized T-shirt\n",
      "Price: ₹449\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-brown-never-give-up-graphic-printed-oversized-t-shirt-628311-1701845554-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Women's Brown Harry's House Graphic Printed Oversized T-shirt\n",
      "Price: ₹319\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-brown-harry-s-house-graphic-printed-oversized-t-shirt-565911-1708436812-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Women's Blue Baggy Wide Leg Jeans\n",
      "Price: ₹1049\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-blue-baggy-wide-leg-jeans-624271-1707215634-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Men's Black Cyber Punk Graphic Printed Oversized T-shirt\n",
      "Price: ₹649\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-cyber-punk-graphic-printed-oversized-t-shirt-627657-1700455717-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Men's Black Ocean Child Graphic Printed Oversized T-shirt\n",
      "Price: ₹619\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-ocean-child-graphic-printed-oversized-t-shirt-610984-1692698147-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Men's Green Cyber Samurai Graphic Printed T-shirt\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-green-cyber-samurai-graphic-printed-t-shirt-589374-1700548868-1.jpg\n",
      "\n",
      "Product name: Bewakoof Air® 1.0Men's Black Oversized Cargo Joggers\n",
      "Price: ₹1049\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-oversized-cargo-joggers-552880-1710766583-1.jpg\n",
      "\n",
      "Product name: bewakoof x dcMen's Black The Dark Knight Graphic Printed T-shirt\n",
      "Price: ₹449\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-the-dark-knight-graphic-printed-t-shirt-592032-1709216951-1.jpg\n",
      "\n",
      "Product name: Bewakoof®Women's Purple Billionaire Girls Club Graphic Printed Oversized T-shirt\n",
      "Price: ₹479\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-purple-billionaire-girls-club-graphic-printed-oversized-t-shirt-604125-1708349620-1.jpg\n",
      "\n",
      "Product name: bewakoof x narutoMen's Black Legendary Sanin Graphic Printed Oversized Acid Wash T-shirt\n",
      "Price: ₹599\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-legendary-sanin-graphic-printed-oversized-t-shirt-628699-1702015876-1.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', class_='productCardBox', limit=10)  # removed extra spaces in class name\n",
    "\n",
    "\n",
    "for product in products:\n",
    "    name_elem = product.find('div', class_='productNaming bkf-ellipsis')\n",
    "    if name_elem:\n",
    "        name = name_elem.text.strip()\n",
    "    else:\n",
    "        name = \"Name not available\"\n",
    "    \n",
    "    price_elem = product.find('div', class_='discountedPriceText')\n",
    "    if price_elem:\n",
    "        price = price_elem.text.strip()\n",
    "    else:\n",
    "        price = \"Price not available\"\n",
    "    \n",
    "    image_url = product.find('img')['src']\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Product name: {name}')\n",
    "    print(f'Price: {price}')\n",
    "    print(f'Image URL: {image_url}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea939518",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da319031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the news articles\n",
    "articles = soup.find_all('div', class_='LatestNews-list')\n",
    "\n",
    "# Loop through the articles and extract the heading, date, and news link\n",
    "for article in articles:\n",
    "    heading = article.find('a',class_=\"LatestNews-headline\").text\n",
    "    date = article.find('time',class_=\"LatestNews-timestamp\")\n",
    "    news_link = article.find('a')['href']\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Heading: {heading}')\n",
    "    print(f'Date: {date}')\n",
    "    print(f'News link: {news_link}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a424302",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c685b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the most downloaded articles\n",
    "articles = soup.find_all('div', class_='article-listing')\n",
    "\n",
    "# Loop through the articles and extract the title, date, and author\n",
    "for article in articles:\n",
    "    title = article.find('h2', class_=\"h5 article-title\").text\n",
    "    date = article.find('p', class_='article-date').text\n",
    "    author = article.find('p', class_='article-authors').text\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Title: {title}')\n",
    "    print(f'Date: {date}')\n",
    "    print(f'Author: {author}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ba184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
